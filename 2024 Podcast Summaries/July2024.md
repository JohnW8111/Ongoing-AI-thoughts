# 1 Latent Space podcast: 10,000x Yolo Researcher Metagame — with Yi Tay of Reka
## AI Podcast with Yi Tay: Notes and Summary

## Introduction

This podcast episode of Latent Space features an in-depth conversation with Yi Tay, formerly the tech lead of PaLM 2 at Google Brain and now chief scientist at Reka.ai. Hosted by Swyx, the discussion covers Yi's career trajectory, his experiences at Google Brain, and his current work at Reka. The conversation delves into various aspects of AI research, model development, and the broader AI landscape, offering insights into the challenges and trends in the field.

## Key Points

1. **Yi Tay's Career Path and Transition to Reka**:
   Yi transitioned from Google Brain to Reka in March 2023. At Google, he was involved in significant projects like PaLM 2, UL2, and Flan. His move to Reka was driven by a desire to experience something new and take on a co-founder role. Yi states, "I identify even today as a scientist and a researcher more than like a startup person."

2. **The Evolution of AI Research Focus**:
   Yi discusses how the field has shifted from task-specific models to more general-purpose ones. He notes, "There was this phrase of like, in 2017, 2018, where this style of work was still very fashionable in academia and conferences, right? And then, I think the big thing about the chat GPT moment of like, 2022, the thing that changed drastically is like, it completely like, it was like this sharp, Make all this work like kind of like obsolete."

3. **Insights on Model Architecture and Efficiency**:
   Yi provides detailed insights into model architectures, particularly the Noam Shazeer architecture. He discusses components like SwiGLU, GQA, and RMSNorm, emphasizing their importance and evolution. On efficiency, Yi states, "I think using active params, I'm comfortable with using active params to kind of approximate like cost of the model, but like in the efficiency misnomer paper, we actually made it quite clear that you should always look holistically about like, because you have serving, like additional serving costs, like fitting in the GPUs, like fitting on single node, and something like that."

4. **Challenges in Large-Scale Model Training**:
   Yi shares the difficulties encountered in training large models, particularly issues with compute reliability. He mentions, "It was very painful because even when the compute came, it was mostly broken most of the time. And it was broken to a very bad extent that, you know, before I left Google I was like, even in the early stage I was very optimistic about You Okay, this compute translates to this amount of flops, this is the model, right? But I never expected the reliability to be so poor that it just threw off all the calculations."

5. **Perspectives on Open Source vs. Closed Models**:
   Yi discusses the debate between open and closed-source models, offering a nuanced view. He states, "I think when most people try to say that like, open source is catching up and everything They kind of mean like, this grassroots, like bottom up people that are like these indie developers that are like, coming together to like, like, fight, like it's romanticized and it's dramatized to some extent."

6. **Long Context vs. Retrieval-Augmented Generation (RAG)**:
   Yi expresses a positive outlook on long context models, stating, "I think long context is definitely the future, rather than RAC, but I mean, they could be used in conjunction." He elaborates on the use cases where long context is preferable, particularly for complex tasks requiring comprehensive understanding.

7. **Efficiency in AI Research**:
   Yi discusses the challenges of evaluating efficiency improvements in AI models. He emphasizes the importance of considering multiple factors beyond just parameter count, stating, "Every time you add complexity, especially if it's like something that's not hardware optimized, no kernels, or like something that is like bad for TPUs or whatever, your model just becomes like slow."

8. **Mixture of Experts (MoE) Models**:
   Yi shares his thoughts on MoE models, seeing them as a promising direction. He states, "I think MOE's are just a trade off with like, prime and flop, right? And then you're able to like, kind of, make, like, you kind of make that. That, that in like that, that scaling log increase from, from that additional."

9. **The Importance of Benchmarks and Evaluations**:
   Yi discusses the challenges with current benchmarks and the need for better evaluation methods. He emphasizes, "A good eval set is one that you don't release... you release some of it, but, like, it's like, you don't, like, you know, let the, like, Let it be contaminated by the community."

10. **Building AI Expertise Outside Silicon Valley**:
    Yi offers insights on developing AI capabilities in places like Singapore. He emphasizes the importance of hands-on expertise, stating, "AI has shifted quite a lot into this IC driven paradigm where the people making impact are the people who are, like, on the ground fighting the war, right? So it's no longer about, like, I have 10 interns, 20 interns, 100 interns, you do this, you do this, you do this, I just take meetings, right?"

## Concise Summary

This podcast offers a comprehensive look into the current state and future directions of AI research and development, as seen through the eyes of Yi Tay, a prominent figure in the field. The conversation covers a wide range of topics, from the technical aspects of model architecture and training to broader issues like the open vs. closed source debate and the challenges of building AI expertise outside of traditional tech hubs. Yi's insights reveal the rapid evolution of AI research, emphasizing the shift towards more general-purpose models and the increasing importance of individual contributors in driving innovation. He highlights the complexities of large-scale model training, the nuances of efficiency in AI systems, and the ongoing challenges in creating meaningful benchmarks. Throughout the discussion, Yi provides a balanced view of the field, acknowledging both the tremendous progress made and the significant challenges that remain. His perspective as someone who has worked in both academic and industry settings, and now in a startup environment, offers a unique and valuable viewpoint on the current state and future potential of AI technology.

# 2. Latent Space podcast: Benchmarks 201: Why Leaderboards > Arenas >> LLM-as-Judge

## Introduction

This episode of the Latent Space podcast features Clémentine Fourrier, a research scientist at Hugging Face and maintainer of the OpenLLM leaderboard. Hosted by Alessio (partner and CTO-in-Residence at Decibel Partners) and Swyx (founder of Smol AI), the discussion delves deep into the intricacies of evaluating large language models (LLMs), the challenges in creating and maintaining benchmarks, and the future of AI model assessment.

## Key Points

1. **OpenLLM Leaderboard**: Clémentine Fourrier maintains the OpenLLM leaderboard at Hugging Face, which has evaluated over 7,400 models. The leaderboard serves as a crucial resource for the AI community, allowing fair comparisons between models and cutting through marketing claims. Fourrier states, "We basically expect the scale of AI progress to go so fast that anyway, we will have to renew them [benchmarks]."

2. **Evaluation Challenges**: The podcast highlights the complexities in evaluating LLMs, including the need for fair and reproducible benchmarks. Fourrier emphasizes the importance of automated benchmarks for their reproducibility, stating, "Automated benchmarks, like the one we're using on the OpenLLM leaderboard, are usually fair and reproducible. Every model gets evaluated in exactly the same way, and you can really reproduce the scores you get."

3. **Human vs. Automated Evaluation**: The discussion covers the pros and cons of human evaluation versus automated benchmarks. Fourrier expresses concerns about human evaluations, particularly "vibe check" evaluations and RNA-type systems, due to potential biases and lack of reproducibility. She notes, "RNAs are not giving you factuality, which should be a super important aspect of LLMs, I think."

4. **Benchmark Selection**: The podcast delves into the process of selecting benchmarks for the OpenLLM leaderboard. Fourrier explains their criteria, including relevance, stability, and community perception. She mentions, "We spent, I'd say, about a month just running the evaluations on a wide variety of models to make sure that the implementations were absolutely correct and fair for all models."

5. **Compute Constraints**: An interesting aspect discussed is the computational resources required for model evaluation. Fourrier reveals, "If we evaluate a 7b model at the moment, it takes approximately two hours. If we evaluate a 70b at the moment, it takes around 20 hours." This highlights the significant computational demands of thorough model evaluation.

6. **Emerging Benchmarks**: The conversation covers new and interesting benchmarks, including GPQA (described as "Basically MMLU, but PhD level") and MUSR (multi-step soft reasoning). Fourrier expresses excitement about benchmarks that models currently struggle with, as they provide room for improvement.

7. **Long Context Evaluation**: The podcast addresses the growing importance of evaluating models on long-context tasks. Fourrier mentions benchmarks like MUSR and a dataset for "learning to translate a new language from one grammar book" as interesting approaches to long-context evaluation.

8. **Agent Benchmarks**: Discussing the evaluation of AI agents, Fourrier highlights the GAIA benchmark she worked on, which tests models in real-world scenarios rather than boxed environments. She expresses hope for more datasets like GAIA, stating, "I really think that anyone could contribute or create similar datasets."

9. **Model Calibration**: Fourrier identifies model calibration as an under-evaluated aspect of LLMs. She explains, "Basically, a model is said to be well calibrated if the log probability score of an answer correlates well with how correct the answer is." She suggests this could lead to models with confidence intervals for their answers.

10. **Future of Evaluation**: Looking ahead, Fourrier mentions working on the next version of the leaderboard. She predicts continued focus on reasoning and math evaluations, exploration of long-context tasks, and possibly the inclusion of code evaluation and psychofancy (how models can be problematic in their interactions) assessments.

## Concise Summary

This podcast episode provides a comprehensive overview of the current state and future directions of LLM evaluation. Clémentine Fourrier offers valuable insights into the challenges and complexities of creating fair, reproducible benchmarks for AI models. The discussion highlights the tension between automated and human evaluations, the computational demands of thorough model assessment, and the constant need to evolve benchmarks as models improve rapidly. 

Key themes include the importance of transparency in model evaluation, the need for diverse and challenging benchmarks, and the emerging focus on long-context tasks and agent capabilities. Fourrier's work on the OpenLLM leaderboard exemplifies the community-driven effort to provide objective comparisons in a field often clouded by marketing claims.

The conversation also touches on future directions, including the need for better calibration metrics, robustness to prompt variations, and assessments of models' tendencies to reinforce user biases. Overall, the podcast underscores the critical role of rigorous evaluation in advancing AI research and development, while highlighting the ongoing challenges in this rapidly evolving field.

Here is a detailed summary of the podcast in markdown format:

# 3 80,000 Hour Podcast Summary:The economy and national security after AGI | Carl Shulman (Part 1)

## Introduction

This podcast features an in-depth discussion between Rob Wiblin and Carl Shulman on the potential economic and societal impacts of advanced artificial intelligence (AI). Carl Shulman, an independent researcher known for his influential work on existential risks, shares his vision of a future where AI systems become capable of performing all human tasks, leading to rapid economic growth and societal transformation. The conversation explores the mechanics of this transition, potential challenges, and ethical considerations surrounding AI development. This episode, part one of a two-part series, focuses primarily on AI's impact on the economy, international conflicts, and the moral status of AI minds.

## Key Points

1. **Rapid Economic Growth Potential**

Carl Shulman argues that once AI systems become capable of performing all human tasks, economic growth rates could increase dramatically. He suggests that the economy could potentially double every couple of months, rather than the current rate of about 5% per year. This acceleration is based on the idea that AI systems could work continuously without rest, rapidly replicate themselves, and operate at much higher efficiency than humans.

"If you do 8,760 hours of the year, 100% employment, at $100 per hour, you're getting close to a million dollars of wages equivalent."

2. **Energy and Resource Utilization**

Shulman discusses the potential for vastly increased energy utilization in an AI-driven economy. He argues that we could potentially harness a much larger portion of the solar energy hitting Earth, leading to enormous increases in available energy per person. This increased energy availability could support a massive expansion of AI "cognitive labor."

"That budget means you could have, per person, an energy budget that can, at any given time, sustain 50,000 human brain equivalents of AI cognitive labour, 10,000 human-scale robots."

3. **Geopolitical Implications**

The conversation explores how rapid AI-driven growth could reshape global power dynamics. Shulman suggests that countries or blocs that develop advanced AI first could gain a decisive strategic advantage, potentially leading to instability or conflicts. He emphasizes the need for international cooperation and agreements to manage this transition safely.

"Even after every state has access to the latest AI technology, a gap in natural resources can remain indefinitely, because right now those sorts of natural resources are too expensive to acquire."

4. **Physical Transformation of the Environment**

Shulman addresses concerns about the physical feasibility of such rapid growth, acknowledging that it would require massive changes to our environment. He argues that the potential economic gains would likely overcome current regulatory and NIMBY-style obstacles to large-scale construction and resource utilization.

5. **AI Capabilities and Human Labor**

The podcast discusses how AI systems could eventually match or exceed human capabilities across all domains. Shulman argues that this would lead to a situation where human labor becomes economically irrelevant, as AI systems could perform all tasks more efficiently and at a lower cost.

"Right now governments redistribute a significant percentage of all of the output in their territories, and we're talking about an expansion of economic output of orders of magnitude."

6. **Income Distribution in an AI-Driven Economy**

Shulman explores potential scenarios for income distribution in a world where AI has made human labor obsolete. He suggests that existing mechanisms of wealth redistribution, if maintained, could ensure a high standard of living for humans even as AI systems dominate the economy.

7. **Moral Status of AI Systems**

The conversation delves into the ethical considerations of creating advanced AI systems. Shulman argues for the importance of considering the potential moral status of AI minds, especially as they become increasingly sophisticated and humanlike in their capabilities.

"It seems pretty likely to me that there will be vast numbers of AIs that are smarter than us, that have desires, that would prefer things in the world to be one way rather than another, and many of which could be said to have welfare."

8. **Challenges in Assessing AI Consciousness**

Shulman discusses the difficulties in determining whether current AI systems have genuine experiences or consciousness. He emphasizes the need for continued research into AI interpretability and suggests that as AI systems become more advanced, these questions will become increasingly pressing.

9. **Coexistence of Humans and AI**

The podcast explores potential scenarios for a future where humans and AI systems coexist. Shulman discusses the challenges of maintaining human relevance and wellbeing in a world where AI systems are vastly more capable, and suggests potential institutional arrangements to ensure human interests are protected.

10. **Need for Proactive Planning and Governance**

Throughout the conversation, Shulman emphasizes the importance of proactive planning and governance to manage the transition to an AI-driven economy. He argues for the development of international agreements, institutional mechanisms, and technological safeguards to ensure that the benefits of AI are broadly shared and potential risks are mitigated.

"The amount of time that we have for human input into that transition is significantly affected by how fast these feedback processes are."

## Concise Summary

This podcast featuring Carl Shulman presents a vision of a future dramatically transformed by advanced artificial intelligence. Shulman argues that once AI systems become capable of performing all human tasks, we could see unprecedented economic growth rates, with the global economy potentially doubling every few months. This rapid growth would be driven by AI systems that can work continuously, replicate quickly, and utilize energy and resources far more efficiently than humans.

The conversation explores the wide-ranging implications of this scenario, from geopolitical power shifts to challenges in wealth distribution and the moral status of AI minds. Shulman emphasizes the need for proactive planning and international cooperation to manage this transition safely and equitably. He also delves into the ethical considerations of creating advanced AI systems, arguing for the importance of considering their potential moral status and welfare.

Throughout the discussion, Shulman presents a nuanced view that acknowledges both the enormous potential benefits of advanced AI and the significant challenges and risks it could pose. The podcast underscores the critical importance of thoughtful governance and foresight in shaping the development of AI technologies to ensure they benefit humanity as a whole.

# 4 80,000 Hour Podcast Summary:Government and society after AGI | Carl Shulman (Part 2)

Here is a detailed summary of the podcast in markdown format:

# AI Advisors and Epistemic Revolution: A Conversation with Carl Shulman

## Introduction

This podcast features an in-depth conversation between Rob Wiblin and Carl Shulman, a polymath researcher known for his influential visions of how superhuman AI might play out. The discussion focuses on the potential impact of AI on government, politics, and decision-making after the development of artificial general intelligence (AGI). Shulman explores how trustworthy superhuman AI advisors could revolutionize governance, the potential for AI to help solve intractable philosophical questions, and the risks and opportunities associated with the rapid advancement of AI technology. This conversation is part of a series, building on previous discussions about the economic and national security implications of AGI.

## Key Points

1. **AI Advisors in Governance**

   Shulman envisions a future where AI advisors could significantly improve policy-making and governance. He argues that AI could provide unbiased, data-driven advice on complex issues, potentially avoiding mistakes made during crises like the COVID-19 pandemic. For example, AI advisors could have helped identify the severity of the outbreak earlier, guided more effective containment strategies, and optimized vaccine development and distribution. Shulman states, "If you have the AI advisors, and they are telling you, 'Look, this stuff is going to happen; you're going to regret it.' The AI advisor is credible. It helps navigate between politicians not fully understanding the economics and politics."

2. **Epistemic Revolution through AI**

   The podcast discusses how AI could lead to an "epistemic revolution" by providing more accurate and unbiased information across various fields. Shulman suggests that AI could help resolve long-standing debates in areas like social science, philosophy, and politics by offering more objective analyses. He explains, "You could see how that could distort things even within the regime. So the Soviet Union collapsed because Gorbachev rose to the top of the system while thinking it was terrible in many ways." This implies that better information and decision-making tools could have profound effects on political systems and ideologies.

3. **Challenges in Trusting AI Advisors**

   Despite the potential benefits, Shulman acknowledges the challenges in getting different parties, especially adversarial ones, to trust the same AI advisor. He discusses the need for transparency in AI development and the importance of having representation from different factions in the creation or auditing of these models. Shulman notes, "For example, Elon Musk, with his Grok AI: the claim is that that is going to be more honest AI and have different political biases than other chatbots... that might be a situation where it makes a big difference whether conservative or Republican legislators or voters in the United States have an AI model that they can to a greater extent trust was not made by their political opponents."

4. **AI in Military and Security**

   The conversation touches on the critical importance of how AI is integrated into military and security services. Shulman emphasizes the need for robust principles and motivations to be embedded in AI systems to prevent misuse or coups. He states, "If you're deploying these powerful AI systems at scale, they're having an enormous amount of influence and power in society — eventually to the point where ultimately the instruments of state hinge on their loyalties — then you really don't want to have this kind of backdoor or password, because it could actually overthrow the government, potentially."

5. **AI and Philosophical Progress**

   Shulman suggests that AI could potentially make more progress in philosophy and other abstract fields than in areas where humans have already made significant advancements. He argues that AI's ability to process vast amounts of information and apply consistent reasoning could lead to breakthroughs in long-standing philosophical debates. Shulman explains, "I think we should separate two things. One is how much absolute progress in knowledge can we generate? And there's some sense in which in the physical sciences we're really great at getting definitive knowledge, and adding in a tonne of research capacity from AI will make that quite a bit better."

6. **AI in Forecasting and Decision-Making**

   The podcast explores how AI could revolutionize forecasting and decision-making. Shulman discusses the potential for AI to provide more accurate predictions about future events, which could significantly impact policy-making and business strategies. He notes, "Creating AIs to forecast economic and political events is something that obviously has huge economic value, by providing signals for financial trading. There is huge social value potentially to be provided by predicting the political consequences and economic consequences of different policies."

7. **Risks of AI Misuse in Ideological Entrenchment**

   Shulman expresses concern about the potential misuse of AI to entrench existing ideologies or beliefs. He discusses the possibility of people using AI to create echo chambers or reinforce their existing worldviews. However, he remains optimistic that the overall effect of AI on society's epistemology will be positive. Shulman states, "My actual best guess is that the result of these technologies comes out hugely in favour of improved epistemology, and we get largely convergence on empirical truth wherever it exists."

8. **International Cooperation on AI Development**

   The conversation touches on the importance of international cooperation in AI development and regulation. Shulman emphasizes the need for agreements between major powers to ensure safe and responsible AI advancement. He suggests that unilateral actions, such as voluntary pauses in AI research, could be counterproductive if they shift relative influence to less cautious actors. Shulman argues, "It seems this would be reducing the slack and intensifying the degree to which international competition might otherwise be close, which might make it more likely that things like safety get compromised a lot."

9. **AI and Societal Values**

   Shulman discusses how AI could influence societal values and potentially help resolve long-standing moral and ethical debates. He suggests that AI's ability to process vast amounts of information and apply consistent reasoning could lead to more objective assessments of ethical issues. However, he also acknowledges the risks of AI being used to reinforce existing biases or ideologies. Shulman notes, "Inevitably, based on history, that will lead to many oxen being gored, and no political or philosophical or religious system or ideology will come out unscathed."

10. **The Future of AI and Human Society**

    In concluding the discussion, Shulman expresses a cautiously optimistic view of the future with AI. While acknowledging the potential risks and challenges, he believes that the overall impact of AI on society could be positive. Shulman states, "My median expectation, I think it's more likely than not that things wind up looking quite good, that we avoid a disaster that kills off humanity, and that probably we don't get a permanent global totalitarianism or something like that." However, he emphasizes the importance of continued efforts to ensure AI development is safe and beneficial to humanity.

## Concise Summary

This podcast explores the potential impact of advanced AI on governance, decision-making, and society at large. Carl Shulman envisions a future where AI advisors could revolutionize policy-making by providing unbiased, data-driven advice on complex issues. He argues that this could lead to an "epistemic revolution," improving our ability to address challenges in various fields, from public health to philosophy. However, Shulman also acknowledges the challenges in implementing such systems, including issues of trust, potential misuse, and the need for international cooperation. The discussion touches on the integration of AI into military and security services, emphasizing the critical importance of embedding robust principles to prevent misuse. Shulman expresses cautious optimism about the future, suggesting that while there are risks associated with AI development, the overall impact could be positive if managed properly. Throughout the conversation, he stresses the need for continued efforts to ensure AI development is safe and beneficial to humanity, highlighting the potential for AI to not only solve practical problems but also to help us address fundamental questions about ethics, values, and the nature of reality.


