# 1 Latent Space podcast: 10,000x Yolo Researcher Metagame — with Yi Tay of Reka
## AI Podcast with Yi Tay: Notes and Summary

## Introduction

This podcast episode of Latent Space features an in-depth conversation with Yi Tay, formerly the tech lead of PaLM 2 at Google Brain and now chief scientist at Reka.ai. Hosted by Swyx, the discussion covers Yi's career trajectory, his experiences at Google Brain, and his current work at Reka. The conversation delves into various aspects of AI research, model development, and the broader AI landscape, offering insights into the challenges and trends in the field.

## Key Points

1. **Yi Tay's Career Path and Transition to Reka**:
   Yi transitioned from Google Brain to Reka in March 2023. At Google, he was involved in significant projects like PaLM 2, UL2, and Flan. His move to Reka was driven by a desire to experience something new and take on a co-founder role. Yi states, "I identify even today as a scientist and a researcher more than like a startup person."

2. **The Evolution of AI Research Focus**:
   Yi discusses how the field has shifted from task-specific models to more general-purpose ones. He notes, "There was this phrase of like, in 2017, 2018, where this style of work was still very fashionable in academia and conferences, right? And then, I think the big thing about the chat GPT moment of like, 2022, the thing that changed drastically is like, it completely like, it was like this sharp, Make all this work like kind of like obsolete."

3. **Insights on Model Architecture and Efficiency**:
   Yi provides detailed insights into model architectures, particularly the Noam Shazeer architecture. He discusses components like SwiGLU, GQA, and RMSNorm, emphasizing their importance and evolution. On efficiency, Yi states, "I think using active params, I'm comfortable with using active params to kind of approximate like cost of the model, but like in the efficiency misnomer paper, we actually made it quite clear that you should always look holistically about like, because you have serving, like additional serving costs, like fitting in the GPUs, like fitting on single node, and something like that."

4. **Challenges in Large-Scale Model Training**:
   Yi shares the difficulties encountered in training large models, particularly issues with compute reliability. He mentions, "It was very painful because even when the compute came, it was mostly broken most of the time. And it was broken to a very bad extent that, you know, before I left Google I was like, even in the early stage I was very optimistic about You Okay, this compute translates to this amount of flops, this is the model, right? But I never expected the reliability to be so poor that it just threw off all the calculations."

5. **Perspectives on Open Source vs. Closed Models**:
   Yi discusses the debate between open and closed-source models, offering a nuanced view. He states, "I think when most people try to say that like, open source is catching up and everything They kind of mean like, this grassroots, like bottom up people that are like these indie developers that are like, coming together to like, like, fight, like it's romanticized and it's dramatized to some extent."

6. **Long Context vs. Retrieval-Augmented Generation (RAG)**:
   Yi expresses a positive outlook on long context models, stating, "I think long context is definitely the future, rather than RAC, but I mean, they could be used in conjunction." He elaborates on the use cases where long context is preferable, particularly for complex tasks requiring comprehensive understanding.

7. **Efficiency in AI Research**:
   Yi discusses the challenges of evaluating efficiency improvements in AI models. He emphasizes the importance of considering multiple factors beyond just parameter count, stating, "Every time you add complexity, especially if it's like something that's not hardware optimized, no kernels, or like something that is like bad for TPUs or whatever, your model just becomes like slow."

8. **Mixture of Experts (MoE) Models**:
   Yi shares his thoughts on MoE models, seeing them as a promising direction. He states, "I think MOE's are just a trade off with like, prime and flop, right? And then you're able to like, kind of, make, like, you kind of make that. That, that in like that, that scaling log increase from, from that additional."

9. **The Importance of Benchmarks and Evaluations**:
   Yi discusses the challenges with current benchmarks and the need for better evaluation methods. He emphasizes, "A good eval set is one that you don't release... you release some of it, but, like, it's like, you don't, like, you know, let the, like, Let it be contaminated by the community."

10. **Building AI Expertise Outside Silicon Valley**:
    Yi offers insights on developing AI capabilities in places like Singapore. He emphasizes the importance of hands-on expertise, stating, "AI has shifted quite a lot into this IC driven paradigm where the people making impact are the people who are, like, on the ground fighting the war, right? So it's no longer about, like, I have 10 interns, 20 interns, 100 interns, you do this, you do this, you do this, I just take meetings, right?"

## Concise Summary

This podcast offers a comprehensive look into the current state and future directions of AI research and development, as seen through the eyes of Yi Tay, a prominent figure in the field. The conversation covers a wide range of topics, from the technical aspects of model architecture and training to broader issues like the open vs. closed source debate and the challenges of building AI expertise outside of traditional tech hubs. Yi's insights reveal the rapid evolution of AI research, emphasizing the shift towards more general-purpose models and the increasing importance of individual contributors in driving innovation. He highlights the complexities of large-scale model training, the nuances of efficiency in AI systems, and the ongoing challenges in creating meaningful benchmarks. Throughout the discussion, Yi provides a balanced view of the field, acknowledging both the tremendous progress made and the significant challenges that remain. His perspective as someone who has worked in both academic and industry settings, and now in a startup environment, offers a unique and valuable viewpoint on the current state and future potential of AI technology.

# 2. Latent Space podcast: Benchmarks 201: Why Leaderboards > Arenas >> LLM-as-Judge

## Introduction

This episode of the Latent Space podcast features Clémentine Fourrier, a research scientist at Hugging Face and maintainer of the OpenLLM leaderboard. Hosted by Alessio (partner and CTO-in-Residence at Decibel Partners) and Swyx (founder of Smol AI), the discussion delves deep into the intricacies of evaluating large language models (LLMs), the challenges in creating and maintaining benchmarks, and the future of AI model assessment.

## Key Points

1. **OpenLLM Leaderboard**: Clémentine Fourrier maintains the OpenLLM leaderboard at Hugging Face, which has evaluated over 7,400 models. The leaderboard serves as a crucial resource for the AI community, allowing fair comparisons between models and cutting through marketing claims. Fourrier states, "We basically expect the scale of AI progress to go so fast that anyway, we will have to renew them [benchmarks]."

2. **Evaluation Challenges**: The podcast highlights the complexities in evaluating LLMs, including the need for fair and reproducible benchmarks. Fourrier emphasizes the importance of automated benchmarks for their reproducibility, stating, "Automated benchmarks, like the one we're using on the OpenLLM leaderboard, are usually fair and reproducible. Every model gets evaluated in exactly the same way, and you can really reproduce the scores you get."

3. **Human vs. Automated Evaluation**: The discussion covers the pros and cons of human evaluation versus automated benchmarks. Fourrier expresses concerns about human evaluations, particularly "vibe check" evaluations and RNA-type systems, due to potential biases and lack of reproducibility. She notes, "RNAs are not giving you factuality, which should be a super important aspect of LLMs, I think."

4. **Benchmark Selection**: The podcast delves into the process of selecting benchmarks for the OpenLLM leaderboard. Fourrier explains their criteria, including relevance, stability, and community perception. She mentions, "We spent, I'd say, about a month just running the evaluations on a wide variety of models to make sure that the implementations were absolutely correct and fair for all models."

5. **Compute Constraints**: An interesting aspect discussed is the computational resources required for model evaluation. Fourrier reveals, "If we evaluate a 7b model at the moment, it takes approximately two hours. If we evaluate a 70b at the moment, it takes around 20 hours." This highlights the significant computational demands of thorough model evaluation.

6. **Emerging Benchmarks**: The conversation covers new and interesting benchmarks, including GPQA (described as "Basically MMLU, but PhD level") and MUSR (multi-step soft reasoning). Fourrier expresses excitement about benchmarks that models currently struggle with, as they provide room for improvement.

7. **Long Context Evaluation**: The podcast addresses the growing importance of evaluating models on long-context tasks. Fourrier mentions benchmarks like MUSR and a dataset for "learning to translate a new language from one grammar book" as interesting approaches to long-context evaluation.

8. **Agent Benchmarks**: Discussing the evaluation of AI agents, Fourrier highlights the GAIA benchmark she worked on, which tests models in real-world scenarios rather than boxed environments. She expresses hope for more datasets like GAIA, stating, "I really think that anyone could contribute or create similar datasets."

9. **Model Calibration**: Fourrier identifies model calibration as an under-evaluated aspect of LLMs. She explains, "Basically, a model is said to be well calibrated if the log probability score of an answer correlates well with how correct the answer is." She suggests this could lead to models with confidence intervals for their answers.

10. **Future of Evaluation**: Looking ahead, Fourrier mentions working on the next version of the leaderboard. She predicts continued focus on reasoning and math evaluations, exploration of long-context tasks, and possibly the inclusion of code evaluation and psychofancy (how models can be problematic in their interactions) assessments.

## Concise Summary

This podcast episode provides a comprehensive overview of the current state and future directions of LLM evaluation. Clémentine Fourrier offers valuable insights into the challenges and complexities of creating fair, reproducible benchmarks for AI models. The discussion highlights the tension between automated and human evaluations, the computational demands of thorough model assessment, and the constant need to evolve benchmarks as models improve rapidly. 

Key themes include the importance of transparency in model evaluation, the need for diverse and challenging benchmarks, and the emerging focus on long-context tasks and agent capabilities. Fourrier's work on the OpenLLM leaderboard exemplifies the community-driven effort to provide objective comparisons in a field often clouded by marketing claims.

The conversation also touches on future directions, including the need for better calibration metrics, robustness to prompt variations, and assessments of models' tendencies to reinforce user biases. Overall, the podcast underscores the critical role of rigorous evaluation in advancing AI research and development, while highlighting the ongoing challenges in this rapidly evolving field.

Here is a detailed summary of the podcast in markdown format:

# 3 80,000 Hour Podcast Summary:The economy and national security after AGI | Carl Shulman (Part 1)

## Introduction

This podcast features an in-depth discussion between Rob Wiblin and Carl Shulman on the potential economic and societal impacts of advanced artificial intelligence (AI). Carl Shulman, an independent researcher known for his influential work on existential risks, shares his vision of a future where AI systems become capable of performing all human tasks, leading to rapid economic growth and societal transformation. The conversation explores the mechanics of this transition, potential challenges, and ethical considerations surrounding AI development. This episode, part one of a two-part series, focuses primarily on AI's impact on the economy, international conflicts, and the moral status of AI minds.

## Key Points

1. **Rapid Economic Growth Potential**

Carl Shulman argues that once AI systems become capable of performing all human tasks, economic growth rates could increase dramatically. He suggests that the economy could potentially double every couple of months, rather than the current rate of about 5% per year. This acceleration is based on the idea that AI systems could work continuously without rest, rapidly replicate themselves, and operate at much higher efficiency than humans.

"If you do 8,760 hours of the year, 100% employment, at $100 per hour, you're getting close to a million dollars of wages equivalent."

2. **Energy and Resource Utilization**

Shulman discusses the potential for vastly increased energy utilization in an AI-driven economy. He argues that we could potentially harness a much larger portion of the solar energy hitting Earth, leading to enormous increases in available energy per person. This increased energy availability could support a massive expansion of AI "cognitive labor."

"That budget means you could have, per person, an energy budget that can, at any given time, sustain 50,000 human brain equivalents of AI cognitive labour, 10,000 human-scale robots."

3. **Geopolitical Implications**

The conversation explores how rapid AI-driven growth could reshape global power dynamics. Shulman suggests that countries or blocs that develop advanced AI first could gain a decisive strategic advantage, potentially leading to instability or conflicts. He emphasizes the need for international cooperation and agreements to manage this transition safely.

"Even after every state has access to the latest AI technology, a gap in natural resources can remain indefinitely, because right now those sorts of natural resources are too expensive to acquire."

4. **Physical Transformation of the Environment**

Shulman addresses concerns about the physical feasibility of such rapid growth, acknowledging that it would require massive changes to our environment. He argues that the potential economic gains would likely overcome current regulatory and NIMBY-style obstacles to large-scale construction and resource utilization.

5. **AI Capabilities and Human Labor**

The podcast discusses how AI systems could eventually match or exceed human capabilities across all domains. Shulman argues that this would lead to a situation where human labor becomes economically irrelevant, as AI systems could perform all tasks more efficiently and at a lower cost.

"Right now governments redistribute a significant percentage of all of the output in their territories, and we're talking about an expansion of economic output of orders of magnitude."

6. **Income Distribution in an AI-Driven Economy**

Shulman explores potential scenarios for income distribution in a world where AI has made human labor obsolete. He suggests that existing mechanisms of wealth redistribution, if maintained, could ensure a high standard of living for humans even as AI systems dominate the economy.

7. **Moral Status of AI Systems**

The conversation delves into the ethical considerations of creating advanced AI systems. Shulman argues for the importance of considering the potential moral status of AI minds, especially as they become increasingly sophisticated and humanlike in their capabilities.

"It seems pretty likely to me that there will be vast numbers of AIs that are smarter than us, that have desires, that would prefer things in the world to be one way rather than another, and many of which could be said to have welfare."

8. **Challenges in Assessing AI Consciousness**

Shulman discusses the difficulties in determining whether current AI systems have genuine experiences or consciousness. He emphasizes the need for continued research into AI interpretability and suggests that as AI systems become more advanced, these questions will become increasingly pressing.

9. **Coexistence of Humans and AI**

The podcast explores potential scenarios for a future where humans and AI systems coexist. Shulman discusses the challenges of maintaining human relevance and wellbeing in a world where AI systems are vastly more capable, and suggests potential institutional arrangements to ensure human interests are protected.

10. **Need for Proactive Planning and Governance**

Throughout the conversation, Shulman emphasizes the importance of proactive planning and governance to manage the transition to an AI-driven economy. He argues for the development of international agreements, institutional mechanisms, and technological safeguards to ensure that the benefits of AI are broadly shared and potential risks are mitigated.

"The amount of time that we have for human input into that transition is significantly affected by how fast these feedback processes are."

## Concise Summary

This podcast featuring Carl Shulman presents a vision of a future dramatically transformed by advanced artificial intelligence. Shulman argues that once AI systems become capable of performing all human tasks, we could see unprecedented economic growth rates, with the global economy potentially doubling every few months. This rapid growth would be driven by AI systems that can work continuously, replicate quickly, and utilize energy and resources far more efficiently than humans.

The conversation explores the wide-ranging implications of this scenario, from geopolitical power shifts to challenges in wealth distribution and the moral status of AI minds. Shulman emphasizes the need for proactive planning and international cooperation to manage this transition safely and equitably. He also delves into the ethical considerations of creating advanced AI systems, arguing for the importance of considering their potential moral status and welfare.

Throughout the discussion, Shulman presents a nuanced view that acknowledges both the enormous potential benefits of advanced AI and the significant challenges and risks it could pose. The podcast underscores the critical importance of thoughtful governance and foresight in shaping the development of AI technologies to ensure they benefit humanity as a whole.

# 4 80,000 Hour Podcast Summary:Government and society after AGI | Carl Shulman (Part 2)

Here is a detailed summary of the podcast in markdown format:

# AI Advisors and Epistemic Revolution: A Conversation with Carl Shulman

## Introduction

This podcast features an in-depth conversation between Rob Wiblin and Carl Shulman, a polymath researcher known for his influential visions of how superhuman AI might play out. The discussion focuses on the potential impact of AI on government, politics, and decision-making after the development of artificial general intelligence (AGI). Shulman explores how trustworthy superhuman AI advisors could revolutionize governance, the potential for AI to help solve intractable philosophical questions, and the risks and opportunities associated with the rapid advancement of AI technology. This conversation is part of a series, building on previous discussions about the economic and national security implications of AGI.

## Key Points

1. **AI Advisors in Governance**

   Shulman envisions a future where AI advisors could significantly improve policy-making and governance. He argues that AI could provide unbiased, data-driven advice on complex issues, potentially avoiding mistakes made during crises like the COVID-19 pandemic. For example, AI advisors could have helped identify the severity of the outbreak earlier, guided more effective containment strategies, and optimized vaccine development and distribution. Shulman states, "If you have the AI advisors, and they are telling you, 'Look, this stuff is going to happen; you're going to regret it.' The AI advisor is credible. It helps navigate between politicians not fully understanding the economics and politics."

2. **Epistemic Revolution through AI**

   The podcast discusses how AI could lead to an "epistemic revolution" by providing more accurate and unbiased information across various fields. Shulman suggests that AI could help resolve long-standing debates in areas like social science, philosophy, and politics by offering more objective analyses. He explains, "You could see how that could distort things even within the regime. So the Soviet Union collapsed because Gorbachev rose to the top of the system while thinking it was terrible in many ways." This implies that better information and decision-making tools could have profound effects on political systems and ideologies.

3. **Challenges in Trusting AI Advisors**

   Despite the potential benefits, Shulman acknowledges the challenges in getting different parties, especially adversarial ones, to trust the same AI advisor. He discusses the need for transparency in AI development and the importance of having representation from different factions in the creation or auditing of these models. Shulman notes, "For example, Elon Musk, with his Grok AI: the claim is that that is going to be more honest AI and have different political biases than other chatbots... that might be a situation where it makes a big difference whether conservative or Republican legislators or voters in the United States have an AI model that they can to a greater extent trust was not made by their political opponents."

4. **AI in Military and Security**

   The conversation touches on the critical importance of how AI is integrated into military and security services. Shulman emphasizes the need for robust principles and motivations to be embedded in AI systems to prevent misuse or coups. He states, "If you're deploying these powerful AI systems at scale, they're having an enormous amount of influence and power in society — eventually to the point where ultimately the instruments of state hinge on their loyalties — then you really don't want to have this kind of backdoor or password, because it could actually overthrow the government, potentially."

5. **AI and Philosophical Progress**

   Shulman suggests that AI could potentially make more progress in philosophy and other abstract fields than in areas where humans have already made significant advancements. He argues that AI's ability to process vast amounts of information and apply consistent reasoning could lead to breakthroughs in long-standing philosophical debates. Shulman explains, "I think we should separate two things. One is how much absolute progress in knowledge can we generate? And there's some sense in which in the physical sciences we're really great at getting definitive knowledge, and adding in a tonne of research capacity from AI will make that quite a bit better."

6. **AI in Forecasting and Decision-Making**

   The podcast explores how AI could revolutionize forecasting and decision-making. Shulman discusses the potential for AI to provide more accurate predictions about future events, which could significantly impact policy-making and business strategies. He notes, "Creating AIs to forecast economic and political events is something that obviously has huge economic value, by providing signals for financial trading. There is huge social value potentially to be provided by predicting the political consequences and economic consequences of different policies."

7. **Risks of AI Misuse in Ideological Entrenchment**

   Shulman expresses concern about the potential misuse of AI to entrench existing ideologies or beliefs. He discusses the possibility of people using AI to create echo chambers or reinforce their existing worldviews. However, he remains optimistic that the overall effect of AI on society's epistemology will be positive. Shulman states, "My actual best guess is that the result of these technologies comes out hugely in favour of improved epistemology, and we get largely convergence on empirical truth wherever it exists."

8. **International Cooperation on AI Development**

   The conversation touches on the importance of international cooperation in AI development and regulation. Shulman emphasizes the need for agreements between major powers to ensure safe and responsible AI advancement. He suggests that unilateral actions, such as voluntary pauses in AI research, could be counterproductive if they shift relative influence to less cautious actors. Shulman argues, "It seems this would be reducing the slack and intensifying the degree to which international competition might otherwise be close, which might make it more likely that things like safety get compromised a lot."

9. **AI and Societal Values**

   Shulman discusses how AI could influence societal values and potentially help resolve long-standing moral and ethical debates. He suggests that AI's ability to process vast amounts of information and apply consistent reasoning could lead to more objective assessments of ethical issues. However, he also acknowledges the risks of AI being used to reinforce existing biases or ideologies. Shulman notes, "Inevitably, based on history, that will lead to many oxen being gored, and no political or philosophical or religious system or ideology will come out unscathed."

10. **The Future of AI and Human Society**

    In concluding the discussion, Shulman expresses a cautiously optimistic view of the future with AI. While acknowledging the potential risks and challenges, he believes that the overall impact of AI on society could be positive. Shulman states, "My median expectation, I think it's more likely than not that things wind up looking quite good, that we avoid a disaster that kills off humanity, and that probably we don't get a permanent global totalitarianism or something like that." However, he emphasizes the importance of continued efforts to ensure AI development is safe and beneficial to humanity.

## Concise Summary

This podcast explores the potential impact of advanced AI on governance, decision-making, and society at large. Carl Shulman envisions a future where AI advisors could revolutionize policy-making by providing unbiased, data-driven advice on complex issues. He argues that this could lead to an "epistemic revolution," improving our ability to address challenges in various fields, from public health to philosophy. However, Shulman also acknowledges the challenges in implementing such systems, including issues of trust, potential misuse, and the need for international cooperation. The discussion touches on the integration of AI into military and security services, emphasizing the critical importance of embedding robust principles to prevent misuse. Shulman expresses cautious optimism about the future, suggesting that while there are risks associated with AI development, the overall impact could be positive if managed properly. Throughout the conversation, he stresses the need for continued efforts to ensure AI development is safe and beneficial to humanity, highlighting the potential for AI to not only solve practical problems but also to help us address fundamental questions about ethics, values, and the nature of reality.

# 5 Cognitive Revolution Podcast:  The State Space Model Revolution, with Albert Gu

## Introduction

This episode of the Cognitive Revolution podcast features an in-depth conversation with Albert Gu, assistant professor at CMU and co-founder of Caresia AI. The main topic is Gu's groundbreaking work on state space models (SSMs) and the Mamba architecture. Host Nathan Labenz explores the intellectual history, technical details, and potential future directions of SSMs. The discussion provides valuable insights into the development of Mamba, its advantages over traditional transformer models, and its implications for the future of AI. This conversation is particularly relevant for those interested in the cutting edge of AI architecture design and the ongoing evolution of language models.

## Key Points

1. **Origins and Motivation of State Space Models**

   Albert Gu's work on state space models (SSMs) began several years ago, inspired by recurrent neural networks (RNNs). He was compelled by the idea of stateful recurrence as a fundamental computational paradigm for modeling sequences. This intuition led to the development of various models, including HIPPO (High-Order Polynomial Projection Operators) and S4 (Structured State Space Sequence Model). Gu emphasizes the importance of compressing context or information into a state, which he believes is crucial for intelligent processing of sequential data.

   > "The idea of compressing context or information into a state is somehow feels important because it's like stripping out unnecessary stuff."

2. **The Mamba Breakthrough**

   The Mamba architecture represents a significant advancement in SSMs. Its key innovation is the introduction of a selection mechanism that allows the model to dynamically adjust how it processes input based on the input itself. This enables the model to focus on important information and ignore irrelevant data, such as filler words in language. The selection mechanism is similar to gating in RNNs but implemented more efficiently.

   > "The idea is like if you have a sequence of tokens there will often be filler tokens or irrelevant tokens in it... The fact that it is taking up a time step is really kind of arbitrary and the idea that we wanted was to be able to skip over time steps if necessary."

3. **State Management in Mamba**

   Gu clarifies that each layer in the Mamba architecture has its own independent state. The state is a fixed-size vector that summarizes everything the model has seen so far. During training, the state doesn't need to be fully materialized due to clever algorithmic techniques. At inference time, the state must be materialized and stored between time steps. The size of the state is a controllable hyperparameter, offering a trade-off between efficiency and performance.

   > "The state is what the model needs to store in memory in between one token to the next token... It really encapsulates the nature of these different types of models."

4. **Mamba 2: Improvements and Trade-offs**

   Mamba 2 introduces changes that make the model more efficient on modern hardware, particularly by leveraging matrix multiplications. This comes at the cost of slightly reduced expressivity in the state update mechanism. However, this trade-off allows for larger state sizes and faster training. Gu notes that the impact of this reduced expressivity is not fully understood and may even provide beneficial inductive bias in some cases.

   > "We were trying to figure out if there was a compromise that could let us keep the spirit of Mamba in these SSMs but find like other algorithmic innovations that could let us leverage matrix multiplications and speed it and like leverage the tensor cores on GPUs."

5. **Comparison with Transformers**

   Gu discusses the fundamental differences between SSMs like Mamba and transformer models. While transformers use attention mechanisms and key-value caches to process all previous tokens, SSMs use a fixed-size state to compress and summarize the history. This leads to different trade-offs in terms of memory usage, computation, and the ability to handle long-range dependencies.

   > "The highest level description of the tradeoff between these things is that the SSM has a fixed size state that has been designed to try to intelligently compress history into a form that is easy for the model to extract new information from."

6. **Applications and Downstream Work**

   The podcast highlights the extensive adoption of Mamba in various fields. Over 267 papers and projects have been derived from Mamba, with applications ranging from language processing to vision, genomics, and time series analysis. Gu notes that SSMs like Mamba seem to perform particularly well on tasks with less structured data or where there hasn't been as much co-evolution with transformer models.

   > "Anywhere you see sequences I think it is kind of a natural choice. I don't think it's always the best choice... but that's hopefully what people have been finding in these follow-up applications."

7. **State Optimization and Creative Uses**

   Gu discusses potential creative uses of the state in SSMs. Unlike the key-value cache in transformers, the SSM state is a more compact and potentially interpretable representation of the input history. This opens up possibilities for tasks like style transfer, speaker encoding, or fine-tuning models by optimizing the initial state rather than the model weights.

   > "You can maybe interpolate things to blend between voices or blend between image styles and stuff like that... I think there's probably so many creative ideas that people can do to leverage the states of these models."

8. **Long Context and Infinite Attention**

   The conversation touches on the potential of SSMs to handle very long contexts. While current models can extrapolate to about 3 times their training context length, Gu believes there's potential for much longer contexts. He emphasizes that true recurrence, as found in SSMs, is necessary for potentially infinite context, although making models actually learn from and utilize such long contexts remains a challenge.

   > "One of the whole reasons that I started this entire line of work on recurrent models is that... it feels like the type of thing that should allow you to get past finite context windows."

9. **Hardware Considerations and Efficiency**

   Gu explains how the design of Mamba and Mamba 2 takes into account modern hardware capabilities, particularly the efficient use of matrix multiplications on GPUs. He discusses the concept of the "hardware lottery" and how the co-evolution of hardware and software can create challenges for new model architectures. The improvements in Mamba 2 aim to strike a balance between leveraging existing hardware optimizations and maintaining the unique benefits of SSMs.

   > "Part of me is a little reluctant to be like let's change the model so that we can make it efficient on hardware because it feels a little bit like playing into the hardware lottery... On the other hand, it's kind of inevitable because the fact is just that like you need something practical to be worthwhile."

10. **Future Directions and Open Questions**

    The podcast concludes with a discussion of potential future directions for SSM research. Gu expresses interest in developing models with multiple specialized states, which could lead to both superior performance and easier interpretability. He also mentions the possibility of more expressive SSM variants, although these would likely come with significant computational costs. The conversation highlights the ongoing need for balancing theoretical advancements with practical considerations in AI research.

    > "I had a vision for this, I have a lot of ideas... but there's still a large sense of it that's kind of intuition driven and it's coming more from the idea that like I don't have all the ideas, there's hopefully going to be plenty of smart people who are interested in this and want to work on things."

## Concise Summary

This podcast episode provides a comprehensive overview of state space models (SSMs) and the Mamba architecture, as explained by their creator, Albert Gu. The discussion traces the development of SSMs from their origins in recurrent neural networks to the latest Mamba 2 model. Key innovations include the ability to compress and selectively process sequential information, offering an alternative to attention-based transformer models. The conversation delves into the technical details of how Mamba manages state, its training and inference processes, and the trade-offs made in Mamba 2 to improve hardware efficiency. Gu emphasizes the complementary nature of SSMs and transformers, suggesting that hybrid approaches may be most effective for various tasks. The wide-ranging applications of Mamba, from language processing to genomics, are highlighted, along with potential future directions such as optimizing state representations and handling extremely long contexts. Throughout the discussion, Gu balances theoretical insights with practical considerations, providing a nuanced view of the challenges and opportunities in advancing AI architectures. The podcast underscores the rapid pace of innovation in AI and the potential for fundamental architectural changes to drive progress in the field.

# 6 Cognitive Revolution Podcast: Building an Intelligent Business OS, with Runway CEO Siqi Chen

## Introduction

In this episode of the Cognitive Revolution podcast, host Nathan Labenz interviews Seiki Chen, founder and CEO of Runway, a finance platform revolutionizing how businesses understand and interact with their financial data through AI. The discussion explores how Runway is integrating AI into their product, their operational use of AI, and Chen's views on the future of AI technology. The conversation delves into the technical aspects of implementing AI in finance, the challenges of making financial data more accessible, and the broader implications of AI advancements for business and society. This 102-minute podcast offers insights into the practical applications of AI in finance and Chen's perspectives on the potential impact of artificial general intelligence (AGI).

## Key Points

1. Runway's Mission and Product
   Runway aims to make business financials more understandable and accessible to everyone in a company, not just the finance team. Chen describes it as a "business operating system" rather than just a finance platform. The product integrates with numerous data sources (about 680) to provide a comprehensive view of a company's operations. Runway uses AI to explain financial concepts, generate scenarios, and make complex financial data more approachable. Chen envisions Runway as a tool that could potentially increase global GDP by enabling better business understanding and decision-making across organizations.

   > "The mission of Runway has never changed. The mission has always been to make business accessible and understandable to everyone."

2. AI Integration in Runway
   Runway incorporates AI in several ways, including an "explain mode" where users can hover over any part of the product to get an explanation, and a scenario planning feature that uses AI to generate financial projections based on user inputs. The AI is designed to work in the background, providing what Chen calls "ambient intelligence" - insights and explanations without the need for explicit user queries. However, Chen acknowledges that the AI's scenario planning capabilities are still in early stages and not entirely reliable yet.

   > "We have this thing called explain mode and what that does is when you hold down the control button and you mouse over any part of the product, it will just tell you what you're looking at and explain it."

3. Challenges in AI Implementation
   Chen discusses the challenges of implementing AI in finance, particularly in making it reliable and trustworthy. He emphasizes that their current AI implementations, especially for scenario planning, are not yet fully dependable and require human oversight. The company focuses on using AI for tasks it's currently good at, such as explaining concepts and making information more accessible, rather than complex decision-making.

   > "I would say we have not tamed it very well at all. This is early and I think the models' capabilities aren't quite there yet. It hallucinates, it will misunderstand, it will get things wrong."

4. Data Integration and Context
   A key aspect of Runway's approach is integrating vast amounts of data from various sources to provide context for financial decisions. This includes not just financial data, but also information from project management tools, CRM systems, and other business software. Chen argues that this comprehensive data integration is crucial for creating a true "simulation" of a business and enabling AI to provide more valuable insights.

   > "Finance is not about really just finance. I think the right way to think about it, the way we think about this, is that it is a simulation of the entire company."

5. AI in Runway's Operations
   Chen shares how Runway uses AI tools internally, including an AI-powered SDR (Sales Development Representative) system for lead qualification and customized email drafting. They also use AI for analyzing job candidate interviews, extracting key information without the need for manual note-taking. These applications demonstrate practical ways AI can enhance operational efficiency in a startup environment.

   > "When our sales team wakes up in the morning, we automatically post that draft in the Gmail draft folder, and so they wake up and in their draft folder is a bunch of pre-written emails, all pre-qualified, all pre-customized, and they just press send."

6. Prompt Engineering and AI Design
   The podcast delves into the importance of prompt engineering and AI design in creating effective AI tools. Chen emphasizes that the quality of AI outputs often depends more on the design of the prompts and the user interface than on the underlying model capabilities. He argues that there's a significant opportunity in applied AI to focus on designing better ways to express AI capabilities to users, rather than just pursuing more powerful models.

   > "Everyone's looking for the more capable model, the better fine-tuning orchestration system, or whatever it is. Reality is, you'll have a few AI labs, and they'll have the most capable models, and there needs to be a lot more work in figuring out how you can use these new AI capabilities and express them to customers in useful ways."

7. Future of AI and AGI
   Chen shares his thoughts on the potential development of artificial general intelligence (AGI) and its implications. He believes that the key to achieving AGI lies in improving AI's ability to self-reflect and improve its own prompts and outputs. Chen suggests that once AI crosses a certain threshold in self-improvement capabilities, it could lead to a rapid acceleration towards AGI.

   > "What we really want to look at is, on the extreme end of capability for a human being... are the human beings incapable of improving the LLMs themselves? And once an LLM or properly orchestrated some kind of eugenic loop can get to that point, then we're there."

8. AI's Impact on Work and Society
   The discussion touches on how AI might change the nature of work and society. Chen believes that even in a world with highly capable AI, there will still be value in human-created work. He suggests that items provably made by humans may increase in value and importance, similar to how handmade artisanal goods are valued today. Chen advises teaching children to be makers and creators, as these skills will likely remain valuable.

   > "Things that are provably made by humans will still be valuable because I think what Charlie Munger observed about the world is something I think about a lot, which is people think that greed drives human progress, but it's not greed, it's actually envy."

9. AI Regulation and Global Competition
   Chen expresses skepticism about the effectiveness of government regulation in AI development. He points out the challenges of regulating a rapidly evolving technology and the potential for regulation to put compliant actors at a disadvantage against less scrupulous competitors. Chen suggests that advancements in areas like mechanistic interpretability might be more effective in addressing AI safety concerns than top-down regulation.

   > "If America regulates compute and research, another nation state will not, and what happens at the critical crossing of capabilities when it gets able to self-bootstrap? So it's a very difficult problem, and the implication of the regulation may not play out the way people want it to."

10. Personal Approach to AI Advancements
    Chen discusses how his views on AI development influence his personal and professional decisions. In terms of investing, he focuses on companies that leverage AI for proactivity and context-richness in new product expressions. For his own company, Runway, he considers the potential for creating an AI "CEO agent" that could understand and manage entire business contexts. However, he acknowledges the unpredictability of a post-AGI world and focuses more on near-term applications and opportunities.

    > "I advise my kids to be makers... learn to code, learn how to make things with their hands, learn how to create games... The question then is like, what is the value of making things if AI can just make it from scratch, from start to finish, instantly, right, in the future?"

## Concise Summary

This episode of the Cognitive Revolution podcast featuring Seiki Chen, CEO of Runway, offers a comprehensive look at the intersection of AI and finance. Chen discusses how Runway is using AI to make financial data more accessible and understandable, aiming to create a "business operating system" that integrates data from hundreds of sources. The conversation covers both the current applications of AI in Runway's product and operations, as well as Chen's thoughts on the future of AI technology.

Key themes include the challenges of implementing reliable AI in finance, the importance of data integration and context, and the role of design in effective AI applications. Chen emphasizes the potential of "ambient intelligence" - AI working in the background to provide insights without explicit queries. The discussion also explores broader implications of AI advancement, including potential impacts on work, society, and global competition in AI development.

Chen's perspective on the path to AGI, focusing on self-improvement capabilities, provides insight into how industry leaders are thinking about long-term AI development. Throughout the conversation, there's a balance between excitement for AI's potential and a pragmatic approach to its current limitations, highlighting the complex landscape of AI integration in business and finance.

# 7 Cognitive Revolution Podcast: Delving into The Prompt Report, with Sander Schulhoff of LearnPrompting.org

## Introduction

This episode of the Cognitive Revolution podcast features Nathan Labenz interviewing Sander Schulhoff, creator of Learn Prompting and organizer of the HackAPrompt contest. The main topic is Schulhoff's recently released "Prompt Report," a comprehensive 78-page survey paper on current prompting techniques for large language models. The discussion covers various aspects of prompt engineering, including best practices for few-shot prompting, challenges in ensembling and evaluation, multilingual and multimodal techniques, AI agents, and automated prompt optimization. Schulhoff also shares insights from leading a large research team and reflects on trust, testing, and project management in complex technical research projects.

## Key Points

1. **Few-Shot Prompting Design Decisions**
   Schulhoff highlights six key pieces of advice for designing few-shot prompts:
   1. Quantity of exemplars: Generally, more exemplars improve performance.
   2. Exemplar ordering: Random ordering is recommended to avoid biases.
   3. Label distribution: Aim for a balanced distribution, unless matching a known imbalanced data distribution.
   4. Exemplar label quality: Ensure correct labeling for best accuracy.
   5. Exemplar format: Choose a common format, such as "Q: [input] A: [output]".
   6. Exemplar similarity: Using similar exemplars to the test instance can improve performance.
   Schulhoff emphasizes the importance of these guidelines, noting that while they generally improve results, they're not guaranteed to work in all cases.

2. **Ensembling Techniques and Open-Ended Tasks**
   The discussion explores the challenges of applying ensembling techniques to open-ended, generative tasks. Unlike classification tasks where majority voting can be effective, generative tasks require more complex approaches. Schulhoff suggests potential strategies:
   - Using a pipeline with multiple stages of generation and criticism
   - Implementing a multi-agent setup with different roles (e.g., writer, grammar checker, style checker)
   - Employing self-criticism techniques based on specific guidelines
   However, Schulhoff notes that these techniques are still evolving and may not yet provide significant benefits over simpler approaches.

3. **Fine-Tuning vs. Pure Prompting**
   The podcast explores the trade-offs between pure prompting techniques and fine-tuning approaches. Schulhoff and Labenz discuss a strategy of starting with a few high-quality, manually created examples, then using these to generate more examples, filtering them, and fine-tuning on the expanded dataset. This approach can potentially yield better results than pure prompting, especially for complex tasks. However, Schulhoff cautions that the effectiveness of this method is not necessarily superior to advanced prompting techniques in all cases.

4. **Multilingual and Multimodal Techniques**
   In the multilingual domain, the discussion highlights that English often performs best, leading to techniques like translating to English, performing the task, then translating back. An interesting approach mentioned is multilingual ensembling, where tasks are run in multiple languages and results are combined.

   For multimodal techniques, Schulhoff highlights "chain of image prompting" as a notable approach. This involves generating and analyzing images as part of the reasoning process, adapting the chain-of-thought concept to visual domains.

5. **AI Agents and Tool Use**
   The podcast addresses the current state of AI agents, acknowledging that while promising, they still face significant challenges in practical applications. Schulhoff expresses excitement about the potential of agents but suggests that advancements in architecture and learning approaches (such as reinforcement learning) may be necessary for more robust and general tool use. He notes that while agents can perform specific tasks well, broader, more flexible capabilities remain elusive.

6. **Automated Prompt Optimization**
   Schulhoff shares a surprising result where an automated system (DSPy) outperformed his manually crafted prompt on a binary classification task. Despite spending 20 hours developing his prompt, the automated system created a more effective prompt using the same training data. This outcome suggests significant potential for automated prompt engineering tools, though Schulhoff notes they typically require ground truth examples to optimize against.

7. **Challenges in Multimodal Prompting**
   The discussion touches on the complexities of prompting for multimodal AI systems, particularly in video generation. Schulhoff notes that prompting for systems like Gen3 by Runway is extraordinarily difficult, requiring very specific and detailed prompts. This echoes the early days of language model prompting, suggesting that multimodal prompting techniques are still in their infancy and require significant expertise to achieve desired results.

8. **Evaluation and Performance Metrics**
   A recurring theme in the podcast is the challenge of evaluating AI performance, especially for open-ended tasks. Labenz mentions that even with structured evaluations, edge cases often arise that aren't captured by automated metrics. Both speakers emphasize the importance of actually using the product and looking at the data regularly, as purely automated approaches for determining improvements are not yet fully reliable.

9. **Research Process and Team Management**
   Schulhoff shares insights from managing a large research team for the Prompt Report project. Key takeaways include:
   - The importance of rigorous testing and CI pipelines to ensure quality
   - The value of conducting a "360 review" for team performance reflection
   - The challenges and time-intensity of systematic literature reviews
   He summarizes his experience with the advice "Trust no one, not even yourself," emphasizing the need for robust processes and checks in complex research projects.

10. **Future of Prompt Engineering**
    The podcast concludes with reflections on the future of prompt engineering as a profession. While automated systems like DSPy show promise in optimizing prompts, challenges remain in generalizing these approaches across diverse domains. Schulhoff expresses uncertainty about the market for automated prompt coaching tools but acknowledges their potential utility. The discussion suggests that while automation may advance, human expertise in prompt engineering is likely to remain valuable, especially for complex, domain-specific applications.

## Concise Summary

This episode of the Cognitive Revolution podcast provides a comprehensive overview of current prompt engineering techniques and research, centered around Sander Schulhoff's extensive "Prompt Report." The discussion covers a wide range of topics, from specific strategies for few-shot prompting to broader challenges in AI development and evaluation. Key themes include the nuances of designing effective prompts, the potential and limitations of automated prompt optimization, and the complexities of working with multimodal and multilingual AI systems. The conversation also touches on the future of AI agents and tool use, highlighting both the exciting possibilities and current limitations in this area. Throughout the podcast, there's a recurring emphasis on the challenges of evaluating AI performance, especially for open-ended tasks, and the importance of hands-on testing and data analysis. Schulhoff's insights from managing a large research project provide valuable lessons on team coordination and quality assurance in AI research. Overall, the podcast offers a deep dive into the state of prompt engineering, balancing technical details with broader implications for AI development and application.



# 8 Prompting Techniques for Large Language Models: A Comprehensive Review

## 1. Introduction

This paper provides an extensive overview of prompting techniques for large language models, covering key concepts, methodologies, and applications.

## 2. Key Concepts and Definitions

### Prompt
Input given to a Generative AI model to guide its output. Can include text, images, audio, etc.

### Prompt Template
A function containing variables that are replaced to create a prompt.

### Prompting Technique
A blueprint for structuring prompts or sequences of prompts.

### Prompt Engineering
The iterative process of developing and refining prompts.

## 3. Important Techniques and Methodologies

### In-Context Learning (ICL)
Providing examples or instructions within the prompt to guide the model's behavior.

#### Example of Few-Shot ICL:
```
2+2: four
4+5: nine
8+0:
```

### Chain-of-Thought (CoT)
Encouraging the model to show its reasoning process.

#### Example of Zero-Shot CoT:
```
Q: Jack has two baskets, each containing three balls. How many balls does Jack have in total?
A: Let's think step by step:
1. Jack has two baskets
2. Each basket contains three balls
3. So for each basket, we have 3 balls
4. To find the total, we multiply: 2 baskets × 3 balls = 6 balls
Therefore, Jack has 6 balls in total.
```

### Decomposition
Breaking complex problems into simpler sub-questions.

### Ensembling
Using multiple prompts and aggregating responses.

### Self-Criticism
Having the model evaluate and improve its own outputs.

## 4. Additional Areas Covered

- Multilingual and multimodal prompting
- Agents and evaluation methods
- Security and alignment concerns
- Benchmarking and case studies

## 5. Significant Findings and Conclusions

- Prompting is a rapidly evolving field with many techniques being developed.
- Different techniques are more effective for different tasks and models.
- Automated prompt engineering tools (like DSPy) show promise for optimizing prompts.
- Security and alignment remain important concerns in prompt engineering.
- A case study on detecting signals of suicide risk highlights the challenges and ethical considerations in applying these techniques to sensitive domains.

## 6. Final Thoughts

This comprehensive review provides a solid foundation for understanding the current state of prompting techniques in AI research and application, emphasizing both the potential and the challenges in this rapidly developing field.
